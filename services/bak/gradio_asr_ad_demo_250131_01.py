"""
æ™ºèƒ½åŒ»ç”Ÿé—®è¯ŠAIç³»ç»Ÿ - Gradioå‰ç«¯
å…¼å®¹ Gradio 6.3.0
"""

import gradio as gr
import requests
import numpy as np
import uuid
import time
import io
from typing import List, Dict
from datetime import datetime
import soundfile as sf

# =====================================================
# é…ç½®å‚æ•°
# =====================================================

BACKEND_URL = "http://localhost:8002"
CHUNK_DURATION = 30  # æ¯30ç§’å‘é€ä¸€æ¬¡éŸ³é¢‘åˆ°åç«¯ï¼ˆæé«˜æœ‰æ•ˆè¯­éŸ³æ—¶é•¿ï¼Œé¿å…SDè¿‡çŸ­æŠ¥é”™ï¼‰
SAMPLE_RATE = 16000
MAX_BUFFER_DURATION = 30  # æœ€é•¿ç¼“å­˜æ—¶é•¿ï¼Œé¿å…æ— é™å¢é•¿
MIN_VOICED_DURATION = 3.0  # æœ€å°æœ‰æ•ˆè¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œä½äºè¯¥å€¼ä¸å‘é€åç«¯
VAD_FRAME_MS = 30          # èƒ½é‡æ£€æµ‹å¸§é•¿ï¼ˆæ¯«ç§’ï¼‰
VAD_ENERGY_THRESHOLD = 0.01  # èƒ½é‡é˜ˆå€¼ï¼ˆç»éªŒå€¼ï¼‰
VAD_WINDOW_DURATION = 8.0   # åªåœ¨æœ€è¿‘çª—å£ä¼°ç®—æœ‰æ•ˆè¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
SILENCE_RESET_SECONDS = 6.0  # è¿ç»­é™éŸ³è¶…è¿‡è¯¥å€¼åˆ™é‡ç½®ç¼“å­˜
MIN_SEND_DURATION = 6.0      # å…è®¸æ›´çŸ­çš„å‘é€çª—å£ï¼ˆç§’ï¼‰
MIN_SEND_INTERVAL = 2.0      # å‘é€æœ€å°é—´éš”ï¼ˆç§’ï¼‰
MIN_VOICED_RATIO = 0.2       # æœ€è¿‘çª—å£å†…è¯­éŸ³å æ¯”é˜ˆå€¼

# Debug
DEBUG_ENABLED = True
DEBUG_HISTORY_LIMIT = 200
DEBUG_SHOW_LINES = 8

# è¯´è¯äººèº«ä»½é…ç½®
MAX_SPEAKER_SLOTS = 10
ROLE_OPTIONS = [
    "åŒ»ç”Ÿ", "æŠ¤å£«", "æŠ€å¸ˆ", "è¯å¸ˆ", "å…¶ä»–(åŒ»é™¢)",
    "æ‚£è€…", "é™ªè¯Š", "å®¶å±", "å…¶ä»–(æ‚£è€…)"
]
HOSPITAL_ROLES = {"åŒ»ç”Ÿ", "æŠ¤å£«", "æŠ€å¸ˆ", "è¯å¸ˆ", "å…¶ä»–(åŒ»é™¢)"}

# =====================================================
# å…¨å±€çŠ¶æ€
# =====================================================

class SessionState:
    def __init__(self):
        self.session_id = None
        self.is_recording = False
        self.is_paused = False
        self.audio_buffer = []
        self.all_segments = []
        self.speaker_mapping = {}
        self.last_send_time = None
        self.chunk_counter = 0
        self.last_voiced_time = None
        self.last_display_count = 0
        self.debug_events = []
        self.last_segment_sig = None
        
    def start_new_session(self):
        self.session_id = f"session_{uuid.uuid4().hex[:8]}_{int(time.time())}"
        self.is_recording = True
        self.is_paused = False
        self.audio_buffer = []
        self.all_segments = []
        self.speaker_mapping = {}
        self.last_send_time = time.time()
        self.chunk_counter = 0
        self.last_voiced_time = time.time()
        self.last_display_count = 0
        self.debug_events = []
        self.last_segment_sig = None
        
        try:
            requests.post(f"{BACKEND_URL}/reset_session", 
                         params={"session_id": self.session_id}, timeout=5)
        except:
            pass

state = SessionState()

# =====================================================
# å·¥å…·å‡½æ•°
# =====================================================

def numpy_to_wav_bytes(audio_data: np.ndarray, sample_rate: int = 16000) -> bytes:
    buffer = io.BytesIO()
    sf.write(buffer, audio_data, sample_rate, format='WAV', subtype='PCM_16')
    buffer.seek(0)
    return buffer.read()


def send_audio_to_backend(audio_data: np.ndarray, session_id: str) -> List[Dict]:
    try:
        wav_bytes = numpy_to_wav_bytes(audio_data, SAMPLE_RATE)
        files = {'file': ('audio.wav', wav_bytes, 'audio/wav')}
        params = {'session_id': session_id}
        
        print(f"Sending {len(audio_data)/SAMPLE_RATE:.1f}s audio to backend...")
        response = requests.post(f"{BACKEND_URL}/asr_sd", files=files, params=params, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            segments = result.get('segments', [])
            print(f"Received {len(segments)} segments")
            return segments
        else:
            print(f"Backend error: {response.status_code}")
            return []
    except Exception as e:
        print(f"Failed to send audio: {e}")
        return []


def debug_log(message: str):
    if not DEBUG_ENABLED:
        return
    ts = datetime.now().strftime("%H:%M:%S")
    state.debug_events.append(f"[{ts}] {message}")
    if len(state.debug_events) > DEBUG_HISTORY_LIMIT:
        state.debug_events = state.debug_events[-DEBUG_HISTORY_LIMIT:]


def debug_summary() -> str:
    if not DEBUG_ENABLED or not state.debug_events:
        return ""
    return "\n".join(state.debug_events[-DEBUG_SHOW_LINES:])


def merge_adjacent_segments(segments: List[Dict]) -> List[Dict]:
    if not segments:
        return []
    
    merged = []
    current = segments[0].copy()
    
    for seg in segments[1:]:
        if seg['speaker_id'] == current['speaker_id']:
            current['end'] = seg['end']
            current['text'] += " " + seg['text']
        else:
            merged.append(current)
            current = seg.copy()
    
    merged.append(current)
    return merged


def get_unique_speakers() -> List[str]:
    speakers = set()
    for seg in state.all_segments:
        speaker_id = seg.get('speaker_id')
        if speaker_id and speaker_id != 'unknown':
            speakers.add(speaker_id)
    return sorted(list(speakers))




def build_conversation_messages(use_mapping: bool = False, use_layout: bool = False):
    """æ„å»º Chatbot æ¶ˆæ¯åˆ—è¡¨"""
    if not state.all_segments:
        return []

    segments = merge_adjacent_segments(state.all_segments)

    messages = []
    for seg in segments:
        speaker_id = seg.get("speaker_id", "unknown")
        text = seg.get("text", "")
        if use_mapping and speaker_id in state.speaker_mapping:
            mapping = state.speaker_mapping[speaker_id]
            display_name = mapping.get("name") or mapping.get("role") or speaker_id
        else:
            display_name = speaker_id
        content = f"{display_name}: {text}".strip()

        if use_layout and use_mapping:
            role = state.speaker_mapping.get(speaker_id, {}).get("role")
            if role in HOSPITAL_ROLES:
                messages.append({"role": "assistant", "content": content})
            else:
                messages.append({"role": "user", "content": content})
        else:
            # æœªè®¾ç½®èº«ä»½å‰ç»Ÿä¸€å·¦ä¾§å±•ç¤º
            messages.append({"role": "assistant", "content": content})

    return messages


def estimate_voiced_duration(audio_data: np.ndarray) -> float:
    """åŸºäºèƒ½é‡çš„ç®€æ˜“VADï¼Œä¼°ç®—æœ‰æ•ˆè¯­éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰"""
    if audio_data.size == 0:
        return 0.0

    frame_len = int(SAMPLE_RATE * VAD_FRAME_MS / 1000)
    if frame_len <= 0:
        return 0.0

    total_frames = int(np.ceil(len(audio_data) / frame_len))
    voiced_frames = 0

    for i in range(total_frames):
        start = i * frame_len
        end = min(start + frame_len, len(audio_data))
        frame = audio_data[start:end]
        if frame.size == 0:
            continue
        energy = float(np.sqrt(np.mean(frame ** 2)))
        if energy >= VAD_ENERGY_THRESHOLD:
            voiced_frames += 1

    voiced_seconds = (voiced_frames * frame_len) / SAMPLE_RATE
    return voiced_seconds


# =====================================================
# æŒ‰é’®äº‹ä»¶å¤„ç†
# =====================================================

def start_consultation():
    state.start_new_session()
    speaker_updates = []
    for _ in range(MAX_SPEAKER_SLOTS):
        speaker_updates.append(gr.update(visible=False))
    speaker_id_updates = [gr.update(value="") for _ in range(MAX_SPEAKER_SLOTS)]
    speaker_role_updates = [gr.update(value=None, interactive=False) for _ in range(MAX_SPEAKER_SLOTS)]
    speaker_name_updates = [gr.update(value="", interactive=False) for _ in range(MAX_SPEAKER_SLOTS)]

    return (
        gr.update(interactive=False),  # start_btn
        gr.update(interactive=True),   # pause_btn
        gr.update(interactive=True),   # end_btn
        [],  # conversation_display
        gr.update(visible=False),  # speaker_settings
        gr.update(interactive=False),  # report_btn
        f"âœ… é—®è¯Šå·²å¼€å§‹ | ä¼šè¯ID: {state.session_id}",
        *speaker_updates,
        *speaker_id_updates,
        *speaker_role_updates,
        *speaker_name_updates
    )


def pause_consultation():
    if state.is_paused:
        state.is_paused = False
        return gr.update(value="â¸ï¸ æš‚åœ"), "â–¶ï¸ é—®è¯Šå·²æ¢å¤"
    else:
        state.is_paused = True
        return gr.update(value="â–¶ï¸ ç»§ç»­"), "â¸ï¸ é—®è¯Šå·²æš‚åœ"


def end_consultation():
    state.is_recording = False
    state.is_paused = False

    conversation_messages = build_conversation_messages(use_mapping=False, use_layout=False)
    state.last_display_count = len(conversation_messages)
    speakers = get_unique_speakers()

    speaker_updates = []
    speaker_id_updates = []
    speaker_role_updates = []
    speaker_name_updates = []

    for i in range(MAX_SPEAKER_SLOTS):
        if i < len(speakers):
            speaker_updates.append(gr.update(visible=True))
            speaker_id_updates.append(gr.update(value=speakers[i]))
            speaker_role_updates.append(gr.update(value=None, interactive=True))
            speaker_name_updates.append(gr.update(value="", interactive=True))
        else:
            speaker_updates.append(gr.update(visible=False))
            speaker_id_updates.append(gr.update(value=""))
            speaker_role_updates.append(gr.update(value=None, interactive=False))
            speaker_name_updates.append(gr.update(value="", interactive=False))
    
    return (
        gr.update(interactive=True),   # start_btn
        gr.update(interactive=False),  # pause_btn
        gr.update(interactive=False),  # end_btn
        conversation_messages,  # conversation_display
        gr.update(visible=True),  # speaker_settings
        gr.update(interactive=False),  # report_btn
        f"ğŸ é—®è¯Šå·²ç»“æŸ | æ£€æµ‹åˆ° {len(speakers)} ä½è¯´è¯äºº",
        *speaker_updates,
        *speaker_id_updates,
        *speaker_role_updates,
        *speaker_name_updates
    )


def apply_speaker_mapping(*inputs):
    """åº”ç”¨è¯´è¯äººèº«ä»½æ˜ å°„ï¼ˆä¸‹æ‹‰é€‰æ‹©ï¼‰"""
    speakers = set(get_unique_speakers())
    total = len(inputs)
    third = total // 3
    speaker_ids = inputs[:third]
    speaker_roles = inputs[third: third * 2]
    speaker_names = inputs[third * 2:]

    state.speaker_mapping = {}
    for spk_id, role, name in zip(speaker_ids, speaker_roles, speaker_names):
        if spk_id in speakers and role:
            state.speaker_mapping[spk_id] = {
                "role": role,
                "name": (name or "").strip()
            }

    conversation_messages = build_conversation_messages(use_mapping=True, use_layout=True)
    state.last_display_count = len(conversation_messages)

    return (
        conversation_messages,
        gr.update(interactive=True),
        f"âœ… å·²åº”ç”¨ {len(state.speaker_mapping)} ä½è¯´è¯äººçš„èº«ä»½æ˜ å°„"
    )


def reset_speaker_mapping():
    state.speaker_mapping = {}
    conversation_messages = build_conversation_messages(use_mapping=False, use_layout=False)
    state.last_display_count = len(conversation_messages)
    return (
        conversation_messages,
        gr.update(interactive=False),
        "ğŸ”„ èº«ä»½æ˜ å°„å·²é‡ç½®"
    )


def generate_report():
    report = f"""# é—®è¯ŠæŠ¥å‘Š

**ä¼šè¯ID:** {state.session_id}
**é—®è¯Šæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**è¯´è¯äººæ•°é‡:** {len(get_unique_speakers())}

## è¯´è¯äººèº«ä»½
"""
    if state.speaker_mapping:
        for k, v in state.speaker_mapping.items():
            role = v.get("role")
            name = v.get("name")
            label = f"{role} {name}".strip()
            report += f"- {k}: {label}\n"
    else:
        report += "æœªè®¾ç½®èº«ä»½æ˜ å°„\n"
    
    report += "\n## å¯¹è¯è®°å½•\n\n"
    
    segments = merge_adjacent_segments(state.all_segments)
    for seg in segments:
        speaker_id = seg['speaker_id']
        if speaker_id in state.speaker_mapping:
            mapping = state.speaker_mapping[speaker_id]
            display_name = mapping.get("name") or mapping.get("role") or speaker_id
        else:
            display_name = speaker_id
        report += f"**{display_name}** [{seg['start']:.1f}s - {seg['end']:.1f}s]\n\n{seg['text']}\n\n"
    
    return report


# =====================================================
# éŸ³é¢‘æµå¤„ç†
# =====================================================

def process_audio_stream(audio):
    prev_sig = state.last_segment_sig
    if not state.is_recording or state.is_paused:
        return gr.update(), "ç­‰å¾…å½•éŸ³..."
    
    if audio is None:
        return gr.update(), "æœªæ£€æµ‹åˆ°éŸ³é¢‘"
    
    sample_rate, audio_data = audio
    
    # è½¬å•å£°é“
    if len(audio_data.shape) > 1:
        audio_data = audio_data.mean(axis=1)
    
    # é‡é‡‡æ ·
    if sample_rate != SAMPLE_RATE:
        ratio = SAMPLE_RATE / sample_rate
        new_length = int(len(audio_data) * ratio)
        audio_data = np.interp(
            np.linspace(0, len(audio_data) - 1, new_length),
            np.arange(len(audio_data)),
            audio_data
        )
    
    # å½’ä¸€åŒ–
    if audio_data.dtype != np.float32:
        audio_data = audio_data.astype(np.float32) / 32768.0

    state.audio_buffer.append(audio_data)
    
    buffer_duration = sum(len(chunk) for chunk in state.audio_buffer) / SAMPLE_RATE
    status_msg = f"å½•éŸ³ä¸­... ç¼“å†²åŒº: {buffer_duration:.1f}s"

    full_audio = np.concatenate(state.audio_buffer)
    # ä»…æ£€æŸ¥æœ€è¿‘çª—å£çš„æœ‰æ•ˆè¯­éŸ³ï¼Œé¿å…é•¿æ—¶é—´é™éŸ³æ‹–ç´¯
    window_samples = int(VAD_WINDOW_DURATION * SAMPLE_RATE)
    if len(full_audio) > window_samples:
        recent_audio = full_audio[-window_samples:]
    else:
        recent_audio = full_audio

    recent_duration = len(recent_audio) / SAMPLE_RATE if len(recent_audio) else 0.0
    voiced_duration = estimate_voiced_duration(recent_audio)
    voiced_ratio = (voiced_duration / recent_duration) if recent_duration > 0 else 0.0
    debug_log(
        f"buffer={buffer_duration:.1f}s recent={recent_duration:.1f}s voiced={voiced_duration:.1f}s ratio={voiced_ratio:.2f}"
    )

    if voiced_ratio >= MIN_VOICED_RATIO:
        state.last_voiced_time = time.time()
    elif state.last_voiced_time and (time.time() - state.last_voiced_time) >= SILENCE_RESET_SECONDS:
        # è¿ç»­é™éŸ³è¿‡ä¹…ï¼Œæ¸…ç©ºç¼“å­˜ï¼Œé¿å…å†å²é™éŸ³æ‹–ç´¯è¯†åˆ«
        state.audio_buffer = []
        state.last_send_time = None
        state.last_voiced_time = None
        debug_log("silence_reset buffer cleared")

    should_send = False
    if (
        voiced_duration >= MIN_VOICED_DURATION
        and voiced_ratio >= MIN_VOICED_RATIO
        and buffer_duration >= MIN_SEND_DURATION
    ):
        if buffer_duration >= CHUNK_DURATION:
            should_send = True
        elif state.last_send_time is None or (time.time() - state.last_send_time) >= MIN_SEND_INTERVAL:
            should_send = True
    debug_log(f"should_send={should_send}")

    if should_send:
        if voiced_duration < MIN_VOICED_DURATION or voiced_ratio < MIN_VOICED_RATIO:
            status_msg = f"âš ï¸ æœ‰æ•ˆè¯­éŸ³ {voiced_duration:.1f}s ä¸è¶³ï¼Œç»§ç»­ç´¯ç§¯ä¸­..."
            max_samples = int(MAX_BUFFER_DURATION * SAMPLE_RATE)
            if len(full_audio) > max_samples:
                state.audio_buffer = [full_audio[-max_samples:]]
            debug_log("skip_send: insufficient voiced")
            return gr.update(), f"{status_msg}\n{debug_summary()}" if DEBUG_ENABLED else status_msg

        status_msg = f"æ­£åœ¨å‘é€éŸ³é¢‘å— #{state.chunk_counter + 1}..."
        audio_to_send = full_audio if buffer_duration >= CHUNK_DURATION else recent_audio
        debug_log(f"send_audio duration={len(audio_to_send)/SAMPLE_RATE:.1f}s")
        segments = send_audio_to_backend(audio_to_send, state.session_id)
        debug_log(f"segments_received={len(segments)}")
        
        if segments:
            state.all_segments.extend(segments)
            state.chunk_counter += 1
            status_msg = f"âœ… å·²æ¥æ”¶å— #{state.chunk_counter} | æ£€æµ‹åˆ° {len(segments)} ä¸ªç‰‡æ®µ"
            overlap_samples = int(0.5 * SAMPLE_RATE)
            if len(audio_to_send) > overlap_samples:
                state.audio_buffer = [audio_to_send[-overlap_samples:]]
            else:
                state.audio_buffer = []
            debug_log(f"total_segments={len(state.all_segments)}")
        else:
            # æœªè¿”å›ç‰‡æ®µæ—¶ï¼Œä¿ç•™ç¼“å­˜ä»¥ç´¯ç§¯æ›´é•¿è¯­éŸ³ï¼ˆé¿å…ä¸¢éŸ³ï¼‰
            status_msg = "âš ï¸ è¯­éŸ³æœ‰æ•ˆæ—¶é•¿ä¸è¶³ï¼Œç»§ç»­ç´¯ç§¯ä¸­..."
            # é™åˆ¶æœ€å¤§ç¼“å­˜é•¿åº¦ï¼Œé˜²æ­¢å†…å­˜å¢é•¿
            max_samples = int(MAX_BUFFER_DURATION * SAMPLE_RATE)
            if len(full_audio) > max_samples:
                state.audio_buffer = [full_audio[-max_samples:]]
            if voiced_ratio < MIN_VOICED_RATIO and state.last_voiced_time is None:
                state.audio_buffer = []
                state.last_send_time = None
                debug_log("no_segments: silent_reset")
            else:
                debug_log("no_segments: keep_buffer")
        
        state.last_send_time = time.time()
    
    messages = build_conversation_messages(use_mapping=False, use_layout=False)
    if state.all_segments:
        last_seg = state.all_segments[-1]
        current_sig = (
            last_seg.get("speaker_id"),
            last_seg.get("start"),
            last_seg.get("end"),
            last_seg.get("text")
        )
    else:
        current_sig = None

    if len(messages) == state.last_display_count and current_sig == prev_sig:
        return gr.update(), status_msg

    state.last_display_count = len(messages)
    state.last_segment_sig = current_sig
    if DEBUG_ENABLED:
        status_msg = f"{status_msg}\n{debug_summary()}"
    return messages, status_msg


# =====================================================
# Gradio ç•Œé¢
# =====================================================

CHAT_CSS = """
#conversation_chatbot {
    height: 520px;
}
#conversation_chatbot .wrap {
    height: 520px;
    overflow-y: auto;
}
"""

with gr.Blocks(title="æ™ºèƒ½åŒ»ç”Ÿé—®è¯ŠAIç³»ç»Ÿ") as demo:
    gr.HTML(
        """
        <script>
        const scrollChatbotToBottom = () => {
            const wrap = document.querySelector('#conversation_chatbot .wrap');
            if (wrap) {
                wrap.scrollTop = wrap.scrollHeight;
            }
        };
        const observer = new MutationObserver(scrollChatbotToBottom);
        const setupObserver = () => {
            const target = document.querySelector('#conversation_chatbot');
            if (target) {
                observer.observe(target, { childList: true, subtree: true });
            }
        };
        window.addEventListener('load', () => {
            setupObserver();
            setInterval(scrollChatbotToBottom, 1000);
        });
        </script>
        """
    )
    gr.Markdown("# ğŸ¥ æ™ºèƒ½åŒ»ç”Ÿé—®è¯ŠAIç³»ç»Ÿ")
    gr.Markdown("åŸºäºè¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ç¦»çš„å®æ—¶é—®è¯Šè®°å½•ç³»ç»Ÿ")
    
    gr.Markdown("""
    <div style="background: #fff3cd; padding: 10px; border-radius: 5px; margin-bottom: 15px;">
        <strong>âš ï¸ ä½¿ç”¨æç¤ºï¼š</strong>
        <ul style="margin: 5px 0;">
            <li>è¯·ä½¿ç”¨ <code>http://localhost:7860</code> æˆ– <code>http://127.0.0.1:7860</code> è®¿é—®ä»¥å¯ç”¨éº¦å…‹é£</li>
            <li>é¦–æ¬¡ä½¿ç”¨æ—¶æµè§ˆå™¨ä¼šè¯·æ±‚éº¦å…‹é£æƒé™ï¼Œè¯·ç‚¹å‡»"å…è®¸"</li>
            <li>ç¡®ä¿åç«¯æœåŠ¡å·²å¯åŠ¨ï¼ˆç«¯å£8002ï¼‰</li>
        </ul>
    </div>
    """)
    
    with gr.Row():
        # å·¦ä¾§
        with gr.Column(scale=2):
            status_display = gr.Textbox(
                label="ç³»ç»ŸçŠ¶æ€",
                value="å‡†å¤‡å°±ç»ªï¼Œç‚¹å‡»ã€Œå¼€å§‹é—®è¯Šã€å¼€å§‹å½•éŸ³",
                interactive=False
            )
            
            with gr.Row():
                start_btn = gr.Button("ğŸ™ï¸ å¼€å§‹é—®è¯Š", variant="primary")
                pause_btn = gr.Button("â¸ï¸ æš‚åœ", interactive=False)
                end_btn = gr.Button("ğŸ ç»“æŸé—®è¯Š", interactive=False)
            
            audio_input = gr.Audio(
                sources=["microphone"],
                streaming=True,
                label="å½•éŸ³è¾“å…¥"
            )
            
            gr.Markdown("### ğŸ’¬ å®æ—¶å¯¹è¯è½¬å½•")
            conversation_display = gr.Chatbot(
                label="å¯¹è¯è®°å½•",
                height=520,
                elem_id="conversation_chatbot"
            )
        
        # å³ä¾§
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ‘¥ è¯´è¯äººèº«ä»½è®¾ç½®")
            
            with gr.Column(visible=False) as speaker_settings:
                gr.Markdown("é—®è¯Šç»“æŸåï¼Œè¯·ä¸ºæ£€æµ‹åˆ°çš„è¯´è¯äººåˆ†é…èº«ä»½ã€‚é€‰æ‹©ä¸ºåŒ»é™¢äººå‘˜çš„æ¶ˆæ¯å°†æ˜¾ç¤ºåœ¨å³ä¾§ï¼Œå…¶å®ƒèº«ä»½æ˜¾ç¤ºåœ¨å·¦ä¾§ã€‚")

                speaker_rows = []
                speaker_id_boxes = []
                speaker_role_dropdowns = []
                speaker_name_inputs = []

                for i in range(MAX_SPEAKER_SLOTS):
                    with gr.Row(visible=False) as row:
                        spk_id = gr.Textbox(label="è¯´è¯äººID", interactive=False)
                        spk_role = gr.Dropdown(
                            choices=ROLE_OPTIONS,
                            label="èº«ä»½é€‰æ‹©",
                            value=None,
                            interactive=True
                        )
                        spk_name = gr.Textbox(
                            label="å§“å",
                            placeholder="ä¾‹å¦‚ï¼šç‹åŒ»ç”Ÿ / æ‚£è€…ææŸ",
                            interactive=True
                        )
                    speaker_rows.append(row)
                    speaker_id_boxes.append(spk_id)
                    speaker_role_dropdowns.append(spk_role)
                    speaker_name_inputs.append(spk_name)

                with gr.Row():
                    apply_btn = gr.Button("âœ… åº”ç”¨è®¾ç½®", variant="primary")
                    reset_btn = gr.Button("ğŸ”„ é‡ç½®")
            
            gr.Markdown("### ğŸ“‹ ç»“æ„åŒ–æŠ¥å‘Š")
            report_btn = gr.Button("ç”ŸæˆæŠ¥å‘Š", interactive=False)
            report_output = gr.Markdown(value="")
    
    # äº‹ä»¶ç»‘å®š
    start_btn.click(
        fn=start_consultation,
        outputs=[start_btn, pause_btn, end_btn, conversation_display,
            speaker_settings, report_btn, status_display,
            *speaker_rows, *speaker_id_boxes, *speaker_role_dropdowns, *speaker_name_inputs]
    )
    
    pause_btn.click(
        fn=pause_consultation,
        outputs=[pause_btn, status_display]
    )
    
    end_btn.click(
        fn=end_consultation,
        outputs=[start_btn, pause_btn, end_btn, conversation_display,
            speaker_settings, report_btn, status_display,
            *speaker_rows, *speaker_id_boxes, *speaker_role_dropdowns, *speaker_name_inputs]
    )
    
    audio_input.stream(
        fn=process_audio_stream,
        inputs=[audio_input],
        outputs=[conversation_display, status_display],
        stream_every=1.0
    )
    
    apply_btn.click(
        fn=apply_speaker_mapping,
        inputs=[*speaker_id_boxes, *speaker_role_dropdowns, *speaker_name_inputs],
        outputs=[conversation_display, report_btn, status_display]
    )
    
    reset_btn.click(
        fn=reset_speaker_mapping,
        outputs=[conversation_display, report_btn, status_display]
    )
    
    report_btn.click(
        fn=generate_report,
        outputs=[report_output]
    )


if __name__ == "__main__":
    print("=" * 60)
    print("æ™ºèƒ½åŒ»ç”Ÿé—®è¯ŠAIç³»ç»Ÿ - Gradio 6.3.0")
    print("=" * 60)
    print("ğŸ“ æœ¬åœ°è®¿é—®: http://localhost:7860")
    print("âš ï¸  éº¦å…‹é£éœ€è¦HTTPSæˆ–localhostè®¿é—®")
    print("=" * 60)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        css=CHAT_CSS
    )